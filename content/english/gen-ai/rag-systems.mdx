---
title: "RAG Systems"
description: "Building retrieval-augmented generation pipelines."
---

# RAG Systems

Retrieval-Augmented Generation combines search with LLMs to ground responses in factual, up-to-date information.

## Architecture Overview

```
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│  Query   │───>│ Embedder │───>│ Vector   │───>│   LLM    │
│          │    │          │    │ Search   │    │ + Context │
└──────────┘    └──────────┘    └──────────┘    └──────────┘
```

## Document Ingestion

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# Chunk documents
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " "],
)
chunks = splitter.split_documents(documents)

# Create vector store
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db",
)
```

## Retrieval & Generation

```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5},
)

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4", temperature=0),
    retriever=retriever,
    return_source_documents=True,
)

result = qa_chain.invoke({"query": "How does authentication work?"})
```

## Summary

- RAG grounds LLM responses in retrieved documents
- Chunk size and overlap significantly impact quality
- Use hybrid search (semantic + keyword) for best results
- Always return source citations for verifiability
